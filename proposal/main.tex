\documentclass{article}

\usepackage{biblatex}

\addbibresource{ref.bib}

\title{ELE 548: Project Proposal}

\author{Calvin Higgins}

\begin{document}

\maketitle

\section{Abstract}

Attaining optimal program performance on modern heterogeneous hardware requires specializing compiler optimizations to individual architectures. To ease the 
engineering burden, researchers have proposed automatically learning optimal orderings of compiler optimization passes. In this work, we minimize (1) binary 
size and (2) mean execution time by ordering LLVM optimization passes with EfficientZero, a sample-efficient reinforcement learning method. We attain an X\% 
reduction in binary size compared to \texttt{-Oz} and an X\% speedup compared to \texttt{-O3} when optimizing the MiBench embedded systems benchmark suite.

\section{Introduction}

The goal of a \textbf{compiler} is simple: find an optimal translation of high-level code into machine code. Compilers begin with a relatively naive 
translation, and iteratively refine it with transformations known as \textbf{optimization passes}. Different optimization pass orders can yield binaries with 
drastically different sizes and runtime performance. The \textbf{phase ordering problem} involves determining the best order to apply a fixed set of 
optimization passes. Historically, expert compiler engineers designed new optimization pass orders for every architecture. However, as hardware rapidly evolves 
and becomes increasingly heterogeneous, maintaining optimization pass orders imposes a growing engineering burden. Automation can alleviate this burden and 
allow compiler experts to focus on more critical tasks~\cite{wang2018}. \\

\noindent
By formulating the phase ordering problem as a \textbf{Markov Decision Process (MDP)}, standard reinforcement learning techniques apply:
\begin{enumerate}
    \item \textbf{State space.} $S$ is the set of all legal programs.
    \item \textbf{Action space.} $A$ is the set of all optimization passes.
    \item \textbf{Transition function.} $T(s, a)$ is the result of applying pass $a$ to program $s$.
    \item \textbf{Reward function.} $R(s, a) = C(s) - C(T(s, a))$ where $C(s)$ is a cost function such as binary size, instruction count, or execution 
    time.
\end{enumerate}
Unfortunately, the phase ordering problem is challenging: the action space $A$ contains hundreds of passes, leading to a combinatorial explosion of possible 
optimization sequences. Moreover, rewards are very sparse as effective sequences are rare. Even evaluating the transition and reward functions is slow as 
they require compiler invocations. \\

\noindent
Accordingly, any learning-based approach to the phase-ordering problem ought to be sample-efficient. CompilerDream~\cite{chaoyi2025}, a state-of-the-art 
phase-ordering policy, adapted DreamerV3~\cite{hafner2025}, a general purpose reinforcement learning method, to compiler optimization. However, although 
DreamerV3 dominates nearly all reinforcement learning methods across a diverse set of domains, it is notably overshadowed by EfficientZero~\cite{ye2021} in low 
sample regimes~\cite{hafner2025}. Motivated by its sample efficiency, I will apply EfficientZero to the phase ordering problem.  

\section{Related Work}

Autophase~\cite{hajali2020}, the first reinforcement learning method for the phase ordering problem, obtained a $1.28\times$ speedup compared to \texttt{-O3} on 
high-level circuit synthesis with proximal policy optimization (PPO). CORL~\cite{mammadli2020} obtained a $1.32\times$ speedup compared to \texttt{-O3} on the 
LLVM test suite with deep Q-learning. POSET-RL\cite{jain2022} also used deep Q-learning to optimize execution time and binary size, achieving a $1.019\times$ 
x86 binary size improvement relative to \texttt{-Oz} on embedded systems benchmarks. More recently, DeCOS~\cite{cui2025} bootstrapped standard reinforcement 
learning methods with large language models, and achieved speedups on SPEC 2017 benchmarks. CompilerDream~\cite{chaoyi2025} pretrained DreamerV3 for code 
optimization, and obtained state-of-the-art results on IR instruction count reduction.

\section{Design}

EfficientZero~\cite{ye2021} is a reinforcement learning method that continues the AlphaGo~\cite{silver2016}, AlphaZero~\cite{silver2018}, 
MuZero~\cite{schrittwieser2020}, and MuZero Reanalyze~\cite{schrittwieser2021} linage. Like MuZero and MuZero Reanalyze, EfficientZero contains three networks:
\begin{enumerate}
    \item \textbf{Representation function.} Maps an observation (program) to a latent state (program representation).
    \item \textbf{Dynamics function.} Maps a latent state (program representation) and an action (optimization pass) to a new latent state 
    (program representation).
    \item \textbf{Prediction function.} Maps a latent state (program representation) to a policy (probability distribution over optimization passes) and a 
    reward (change in binary size).
\end{enumerate}
By mapping observations into a latent space with the representation function, EfficientZero can use the dynamics and prediction functions to explore the 
state space without executing actions on the real state space. In the context of phase ordering, this minimizing compiler invocations. EfficientZero also 
includes several other training tricks on top of MuZero Reanalyze to accelerate convergence. \\

\noindent
Initially, I will pre-train EfficientZero on AnghaBench~\cite{dasilva2021}, a suite of 1,041,333 compile-only C/C++ functions scraped from GitHub. AnghaBench is
large-scale, diverse and human-generated, but many programs are too simple to afford significiant optimization opportunities~\cite{chaoyi2025}. Time permitting,
I will pre-train on CodeContests~\cite{li2022} instead, as recommended by the authors of CompilerDream~\cite{chaoyi2025}.

\section{Evaluation}

% TODO: mention LLVM 
CompilerGym~\cite{cummins2022} is a set of environments for compiler optimization tasks such as ordering optimization passes to reduce binary size and 
instruction counts. Since there is limited support for optimizing execution time, I will minimize x86-64 binary size by ordering optimization passes from the 
LLVM~\cite{lattner2004} compiler infrastructure. In particular, I will evaluate my method on binary size improvement factor relative to \texttt{-Oz}, the \
human-designed aggressive binary size optimization sequence, on the MiBench embedded systems benchmark suite\cite{guthaus2001}. Time permitting, I will explore 
other datasets like cBench~\cite{fursin2014}, stronger baselines like random search, other objectives like minimizing mean execution time and other platforms 
like ARM64.

\printbibliography

% survey paper: https://dl.acm.org/doi/pdf/10.1145/3197978?casa_token=-DDX5GUdAXoAAAAA:T2rg1-lxhWKAvF0o6LmjCqe4xmM_-JrdH5EjDbTANo9WqlPdpZEJCSNgguREu1zbFkbMrwEq_irI
% architecture survey paper: https://dl.acm.org/doi/pdf/10.1145/3494523?casa_token=jvcb3B5pe9QAAAAA:f2v9warN-BIT0uLMSI0UC0skUiJOKioEKHUyY4rZoMpyV5-E31nV80RjdD3NUMKZfelu1Tzg1shV

% DL models:
% https://dl.acm.org/doi/10.1145/3640537.3641582
% RL + LLM: https://hpcrl.github.io/ICS2025-webpage/program/Proceedings_ICS25/ics25-26.pdf

% non-DL models:
% https://dl.acm.org/doi/pdf/10.1145/3715756
% https://dl.acm.org/doi/10.1145/3735452.3735530

% more flexible than opt passes: https://dl.acm.org/doi/pdf/10.1145/3696443.3708922
% https://arxiv.org/pdf/2410.03120v2
% register allocation: https://dl.acm.org/doi/pdf/10.1145/3578360.3580273

% compilergym for architecture: “ArchGym: An open-source gymnasium for machine learning assisted architecture design,
% compilergym imporvement: https://dl.acm.org/doi/pdf/10.1145/3640537.3641580

% vision paper (cummins): https://scontent-bos5-1.xx.fbcdn.net/v/t39.8562-6/240841093_348724053629846_1551966423153256824_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=e280be&_nc_ohc=SBNzHCt9NUoQ7kNvwHMzHwR&_nc_oc=AdmwqWSAXj0GwWozUgvfgjldeKBXzA0ZPg1Pytsa3n1GEdBsWjMZ968aHmlnh9MfeStWISh05txE9_6FIqqjqNCP&_nc_zt=14&_nc_ht=scontent-bos5-1.xx&_nc_gid=rvUDSW0JDs3fG_3T1T4RRg&oh=00_AfcfOmoYuZ7ggtc5oyq7FQyeOq7mT-cDF7abi7UNmOX2lg&oe=68F5FD6B

% find heuristics in code, optimize them: https://dl.acm.org/doi/10.1109/CGO57630.2024.10444847

% polyhedral: https://dl.acm.org/doi/10.1145/3721145.3725766

% find performance bugs in compilers by running LLM code: https://dl.acm.org/doi/pdf/10.1145/3708493.3712686


% 5 minutes -> 5 slides

% Attaining optimal program performance on modern heterogeneous hardware requires specializing compiler optimizations to individual architectures. To ease the 
% engineering burden, researchers have proposed automatically learning optimal orderings of compiler optimization phases. In this work, we minimize (1) binary 
% size and (2) mean execution time by ordering LLVM optimization passes with EfficientZero, a sample-efficient reinforcement learning method. We attain an X\% 
% reduction in binary size compared to \texttt{-Oz} and an X\% speedup compared to \texttt{-O3} when optimizing the MiBench benchmark suite.


% hardware, compilers
% phase ordering
% 3x efficientzero

%     muzero reanalyze

%     3 networks:
%         representation function: state + optional history to latent state
%         dynamics function: latent state + action to latent state + reward
%         prediction function: latent state to policy (prob dist over actions) + value (expected reward from policy)

%     MCTS:
%         predicted policy, value should be consistent with later policy, value after rollout
%         make policy, values consistent
    
%     to train reward:
%         do a rollout
%         pick state
%         map to latent state
%         rollout the next n states
%         train reward to match the actual reward n steps out

%     to train value:
%         same
%         train value to match sum of rewards n steps out, plus value at final state


%     efficientzero
%         1. should be able to predict real t+1 latent state (t+1 state to latent) via latent t + transition function
%         2. instead of predicting reward in dynamics function, predict sum of next m rewards
%         3. when retraining on old sequences, use shorter rollouts for older sequences
    

% EfficientZero: How It Works: 
% https://www.lesswrong.com/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works

% Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMind’s Innovations
% https://arxiv.org/pdf/2502.10303
% Autophase 2
% https://chriscummins.cc/pub/2022-autophase-v2.pdf

% How to write an abstract
% https://users.ece.cmu.edu/~koopman/essays/abstract.html

% MLGO: targets code size via inlining
% https://arxiv.org/pdf/2101.04808

\end{document}
