\section{Methods}

\subsection{Program Representation}

We based our program representation on AutoPhase features~\cite{hajali2020}. AutoPhase features are a collection of 56 features statically extracted from LLVM 
IR including basic block counts, constant counts, branch counts, and instruction counts. As feature scales vary, we experimented with three different approaches
to feature standardization: (1) no standardization, (2) Welford's online standardization algorithm, and (3) an autoencoder. In (2), we estimate feature mean and
variance online, and use these estimates for standardization. In (3), we embed AutoPhase features with a simple feed-forward autoencoder, and use it as the 
program representation. The autoencoder is learned online using a linear combination of the agent loss and the autoencoder loss. We also attempted to evaluate 
inst2vec~\cite{bennun2018} embeddings but a performance bug in the CompilerGym framework made it infeasible.

\subsection{Deep Q-Learning}

We aim to minimize IR instruction count. That is, the cost function $C(s)$ is the number of IR instructions. We consider two reward functions: (1) 
the change in IR instruction count $C(s_t) - C(s_{t-1})$ and (2) the signed change in IR instruction count $\mathrm{Sign}(C(s_t) - C(s_{t-1}))$. For (1), we 
also considered online standardization with Welford's algorithm.

We train a neural network, known as a deep Q-network (DQN), to approximate the Q-function
\begin{align*}
    Q(s, a) = R(s, a) + \gamma \max_{a'} Q(T(s, a), a')
\end{align*}
which is the maximum discounted possible reward from taking action $a$ in state $s$. By inspecting the value of the Q-function for each action, we can select 
the best optimization pass to apply at each timestep. Training consists of two alternating phases: rollout and replay. 

\begin{figure}
    \centering
    \label{fig:dqn_rollout}
    \includegraphics[scale=0.18]{dqn_rollout.png}
    \caption{Rollout phase of deep Q-learning. New training examples are generated by greedily sampling from the DQN.}
\end{figure}

Figure~\ref{fig:dqn_rollout} illustrates the rollout phase. Given a batch of programs, we follow an epsilon-greedy policy: either we greedily sample 
optimization passes with the DQN, or we randomly select optimization passes. The program representation before and after each optimization pass is saved to a 
FIFO memory alongside the optimization pass and change in IR instruction count. 

\begin{figure}
    \centering
    \label{fig:dqn_replay}
    \includegraphics[scale=0.18]{dqn_replay.png}
    \caption{Replay phase of deep Q-learning. The predicted value of taking a given action in a given state is updated to align with the actual value of taking 
    the action and the predicted remaining value.}
\end{figure}

Figure~\ref{fig:dqn_replay} illustrates the replay phase. We randomly sample a batch of new and old program representations, selected optimization pass and 
change in IR instruction count from the memory. We estimate the expected remaining IR instruction count reduction from the new program representation using the 
DQN, and then train it to predict the sum of the change in IR instruction and the expected remaining IR instruction count reduction from the old program 
representation and selected optimization pass. We perform soft updates with a policy and target network for stability.

\begin{figure}
    \centering
    \label{fig:dqn_replay_autoencoder}
    \includegraphics[scale=0.36]{dqn_replay_autoencoder.png}
    \caption{Replay phase of deep Q-learning with an autoencoder. The corresponding rollout phase is similar.}
\end{figure}
